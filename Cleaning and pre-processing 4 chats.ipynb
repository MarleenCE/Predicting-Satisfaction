{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e06adbd1-2cd0-4b96-ad18-33c81c2aa9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import smogn\n",
    "import re\n",
    "import random\n",
    "from smogn import smoter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob\n",
    "from textblob_nl import PatternTagger, PatternAnalyzer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "import optuna.visualization as vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "id": "857c0c03-77da-4403-b0bf-e30e9229d6f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data cleanedv2.csv', header = 0, sep= ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b5e4d-ed79-494c-a41f-0269bfe7d62d",
   "metadata": {},
   "source": [
    "# Dataset cleaning and pre-processing for max. 4 chats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1243d9-92ac-48af-9dff-42050db4e98f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aaf1db-610e-4f5d-a948-049226688f13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Extracting Travel Month feature from Arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "id": "bc481fce-7e4c-42a7-bb4e-857e341c3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Aankomst'] = pd.to_datetime(df['Aankomst'], format='%d/%m/%Y') #feature extraction\n",
    "\n",
    "# Extracting the month and creating the 'Travel Month' column\n",
    "df['Travel Month'] = df['Aankomst'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0eb22-a936-44b1-a2c5-8ffe2f31f7be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dropping Features that are not used, Duplicates and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "id": "5ec5f065-09ce-4794-b8c7-31d7d8b18d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Received Messages', 'Aankomst', 'Vertrek', 'Boekingsnummer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "id": "567f8660-2945-4eef-b90d-52a3498e8c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_na(dataset, column):\n",
    "    \"\"\"\n",
    "    Drops rows with missing values in the specified column from the dataset\n",
    "\n",
    "    Parameters:\n",
    "    dataset: The dataset containing the data (df)\n",
    "    column: The name of the column where missing values should be checked and removed (str)\n",
    "    \"\"\"\n",
    "    dataset.dropna(subset=[column], inplace=True)\n",
    "\n",
    "drop_na(df, 'Grade')\n",
    "drop_na(df, 'Gespreksdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729a3a0-c60f-41e5-b183-0e4a8abea3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates in the dataset\n",
    "\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f24475-4cb1-4f3b-9950-6329c83bf72c",
   "metadata": {},
   "source": [
    "### Cleaning Text Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "id": "4b85b8ac-72ab-498a-b8cb-e697353251fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"en\", \"is\", \"zijn\", \"was\", \"oke\", \"ok\", \"of\", \"dat\", \"voor\",\n",
    "                 \"ons\", \"naar\", \"maar\", \"dus\", \"die\", \"bij\", \"een\", \"hebben\", \"dan\", \"mee\", \"daar\",\n",
    "                 \"is\", \"heb\", \"zou\", \"wat\", \"kan\", \"aan\", \"iets\", \"hier\",\n",
    "                 \"met\", \"moet\", \"gaan\", 'deze', 'graag', 'alle', 'zeker', 'maken', 'nog', 'hoor', 'al', 'zouden', 'vanaf',\n",
    "                 'toch', 'jij', 'zitten', 'waar', 'meer', 'gedaan', 'ben', 'geven', 'even', 'als', 'alles', 'doen', 'via', \n",
    "                 'kunnen', 'jullie', 'onze', 'door', 'mag', 'willen', 'staan', 'weet', 'krijgt', 'houden', 'gaat',\n",
    "                 'geef', 'zien', 'daarna', 'wij', 'weten', 'komen', 'omdat', 'mijn', 'op', 'de', 'aan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "id": "294ea61f-72d2-431d-abe8-10f60406f48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Cleans the chat message feature by removing single characters and stopwords.\n",
    "\n",
    "    Parameters:\n",
    "    text: The input feature consisting of text to be cleaned (str)\n",
    "\n",
    "    Returns:\n",
    "    Cleaned text (str)\n",
    "    \"\"\"\n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if len(word) > 1)\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    return text\n",
    "\n",
    "df['Gespreksdata'] = df['Gespreksdata'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "id": "44f6b15e-ed21-4945-97d6-4447b332e328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop_messages(text):\n",
    "    \"\"\"\n",
    "    Removes messages after the first four messages received from the customer.\n",
    "\n",
    "    Parameters:\n",
    "    text: The input feature consisting of text to be cleaned (str)\n",
    "\n",
    "    Returns:\n",
    "    Cleaned text (str)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the pattern to match the time and date format\n",
    "    pattern = r'\\d{2}:\\d{2}:\\d{2} \\d{2}-\\d{2}-\\d{4} \\d+:'\n",
    "    # Use regular expression to split the text into messages\n",
    "    messages = re.split(pattern, text)\n",
    "    \n",
    "    # Remove messages like \"sophie aan\", \"louise aan\", \"tess aan\" and messages shorter than 1 token\n",
    "    filtered_messages = [message for message in messages if not re.match(r'^[A-Z\\s]+AAN\\s', message) and len(message.split()) > 1]\n",
    "    \n",
    "    # Keep only the first 4 messages\n",
    "    cropped_messages = ''.join(filtered_messages[:4])\n",
    "    return cropped_messages\n",
    "\n",
    "df['Gespreksdata'] = df['Gespreksdata'].apply(crop_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614d67c-4618-4835-b657-d7b40369e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the remaining four messages by removing non-alphabetic characters.\n",
    "\n",
    "    Parameters:\n",
    "    text: The input feature consisting of text to be cleaned (str)\n",
    "\n",
    "    Returns:\n",
    "    Cleaned text (str)\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_text = ''.join(char.lower() if char.isalpha() or char.isspace() else ' ' for char in text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "df['Gespreksdata'] = df['Gespreksdata'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5ca23-7090-4732-a045-80b8193c450c",
   "metadata": {},
   "source": [
    "### Extracting Sentiment Score from Chat Message feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "id": "ed030889-06d6-4512-a3d3-a0d265d485b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_sentiment_score(text):\n",
    "    blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "    return blob.sentiment[0]\n",
    "\n",
    "def compute_subjectivity_score(text):\n",
    "    blob = TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "    return blob.sentiment[1]\n",
    "\n",
    "df['sentiment_score'] = df['Gespreksdata'].apply(compute_sentiment_score)\n",
    "df['subjectivity_score'] = df['Gespreksdata'].apply(compute_subjectivity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770b3f2-f08d-40b7-bf05-8845bb2a1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['subjectivity_score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "id": "6fdb5ad5-1cde-4253-97c8-9696d08c5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Land'] = df['Land'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b30f31f-85f3-4f51-8c1a-3209beac8679",
   "metadata": {},
   "source": [
    "### Rounding Grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "id": "1bdc895a-98e0-41f1-aecb-707cb84c9541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Round grades to the nearest half integer\n",
    "df['Rounded_Grade'] = df['Grade'].apply(lambda x: 1.0 if x == 0.0 else round(x * 2) / 2)\n",
    "\n",
    "grade_counts = df['Rounded_Grade'].value_counts()\n",
    "\n",
    "# Filter out rows with rounded grades that only exist once\n",
    "df_filtered = df[df['Rounded_Grade'].isin(grade_counts[grade_counts > 1].index)]\n",
    "\n",
    "df_filtered.drop(columns=['Grade'], inplace=True)\n",
    "df_filtered.rename(columns={'Rounded_Grade': 'Grade'}, inplace=True)\n",
    "df = df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "id": "0ccc56bf-f375-4530-be8d-1eae6c65d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54ca01-216c-4147-9c3f-c034ad53ef91",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "id": "6d989c44-f6ba-4e39-a79a-eb4b48278e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df['Grade']\n",
    "train, test = train_test_split(df, test_size=0.15, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c433d-ab82-46cb-90d9-0e763f73dce0",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd635623-47e1-4785-a083-97b342b68c59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Undersampling + Synonym oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "id": "fe378661-c03a-4c34-9526-5985f3bba763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where Grade is lower than 7\n",
    "subset_train = train[train['Grade'] < 7]\n",
    "\n",
    "# Load synonyms data\n",
    "synonyms_data = pd.read_csv('synonyms.tsv', sep='\\t', header=None, names=['Word', 'Synonym1', 'Synonym2', 'Synonym3', 'Synonym4', 'Synonym5'], usecols=[0, 1, 2, 3, 4, 5])\n",
    "\n",
    "# Create a dictionary of synonyms\n",
    "synonyms_dict = {}\n",
    "for index, row in synonyms_data.iterrows():\n",
    "    word = row['Word']\n",
    "    synonyms = row.dropna().tolist()[1:]  # Exclude the first column (Word)\n",
    "    synonyms_dict[word] = synonyms\n",
    "\n",
    "# Iterate over rows in subset_df and replace words with synonyms\n",
    "for index, row in subset_train.iterrows():\n",
    "    text_feature = row['Gespreksdata']\n",
    "    tokens = text_feature.split()\n",
    "    new_text = []\n",
    "    for token in tokens:\n",
    "        if token in synonyms_dict:\n",
    "            synonym = random.choice(synonyms_dict[token])\n",
    "            new_text.append(synonym)\n",
    "        else:\n",
    "            new_text.append(token)\n",
    "    new_text_feature = ' '.join(new_text)\n",
    "    subset_train.at[index, 'Gespreksdata'] = new_text_feature\n",
    "\n",
    "# Concatenate subset_df and df\n",
    "train = pd.concat([subset_train, train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "id": "abe54754-a705-40de-960b-c2b4e8928801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade\n",
      "8.0     1524\n",
      "7.0     1201\n",
      "9.0      995\n",
      "6.0      870\n",
      "10.0     803\n",
      "5.0      450\n",
      "1.0      318\n",
      "4.0      242\n",
      "3.0      182\n",
      "2.0      156\n",
      "6.5      128\n",
      "8.5      114\n",
      "7.5       61\n",
      "9.5       26\n",
      "5.5       12\n",
      "2.5        4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# RANDOM UNDERSAMPLING\n",
    "\n",
    "train['above_7'] = (train['Grade'] >= 7).astype(int)\n",
    "\n",
    "X = train.drop(columns=['above_7'])\n",
    "y = train['above_7']\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "y_resampled_df = pd.DataFrame(y_resampled, columns=['above_7'])\n",
    "\n",
    "# Merge X and y back together\n",
    "resampled_data = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
    "\n",
    "train = resampled_data.drop(columns=['above_7'])\n",
    "\n",
    "print(train['Grade'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3361c-5f6e-4f59-bae2-eeedf88faf53",
   "metadata": {},
   "source": [
    "## Save Train and Test set\n",
    "These resampled train and test set, with a maximum of 4 chats will be used for all models with experiments for max. 4 chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "id": "d0e222cb-25a4-4d65-a085-718ad1622d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train4.csv', index=False)\n",
    "test.to_csv('test4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
